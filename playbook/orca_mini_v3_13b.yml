---
- name: Setup Default 
  import_playbook: default.yml

- name: Install Orca mini v3 13B GGML for use with llama.cpp
  hosts: all
  vars:
    model_dir: "{{ llm_model_dir }}/orca_mini_v3_13b"
    aria2_list: "model.list"
    llama_dir: "{{ llm_git_dir }}/llama.cpp"

  roles:
    - name: Install llama.cpp
      role: llama.cpp
      vars: 
        llama_dir: "{{ llm_git_dir }}/llama.cpp"

  tasks:
    - name: Verify variables
      debug: 
        msg:
         - "llama_dir: {{ llama_dir }}"
         - "src_model_dir: {{ src_model_dir }}"
         - "model_dir: {{ model_dir }}"
         - "aria2_list: {{ aria2_list }}"

    - name: Create Model dir
      file:
        path: "{{ model_dir }}"
        state: directory

    - name: Verify deployed modle file list
      file:
        path: "{{ model_dir }}/{{ aria2_list }}"
        state: file

    - name: Fetch Model Parameters
      shell:
        cmd: "aria2c --input-file '{{ aria2_list }}' -x 16 -j 16 -s 16 -c"
        chdir: "{{ model_dir }}"

    - name: Example usage
      debug:
        msg: 
          - "cd {{ llama_dir }}"
          - "python3 -m pip install numpy"
          - "./convert-llama-ggmlv3-to-gguf.py --input {{ model_dir }}/orca_mini_v3_13b.ggmlv3.q8_0.bin --output {{ model_dir }}/orca_mini_v3_13b.gguf.q8_0.bin"
          - "./main -ngl <LAYERS> --threads 14 --model {{ model_dir }}/orca_mini_v3_13b.gguf.q8_0.bin --file {{ llm_prompt_dir }}/instruct/ansible.txt"
        
